%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{S:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Intelligible speech can be defined as the extent in which the elements in a speaker's acoustic signal, e.g. phonemes or words, can be correctly recovered by a listener \citep{Freeman_et_al_2017, Kent_et_al_1989, vanHeuven_2008, Whitehill_et_al_2004}. Intelligible spoken language carries an important societal value, as its attainment requires all core components of speech perception, cognitive processing, linguistic knowledge, and articulation to be mastered \citep{Freeman_et_al_2017}. In that sense, \textit{speech intelligibility} is considered a milestone in children's language development, and more practically, it is qualified as the ultimate checkpoint for the success of speech therapy, and the `gold standard' for assessing the benefit of cochlear implantation \citep{Chin_et_al_2012}. 

Multiple approaches can be taken to quantify (measure) \textit{speech intelligibility} \citep{Boonen_et_al_2020, Boonen_et_al_2021, Flipsen_2006, Hustad_et_al_2020}, but among them, \textit{objective rating} methods on stimuli recovered from spontaneous speech tasks have received special attention \citep{Boonen_et_al_2021, Hustad_et_al_2020}. In objective rating methods, listeners transcribe children's utterances orthographically (or phonetically), and use such information to construct an intelligibility score. The construction of the score can be done in many ways, e.g. counting and normalize the number of (un)intelligible syllables or words in the utterances \citep{Flipsen_2006, Lagerberg_et_al_2014}, or calculating the transcriptions' entropy, a measure that expresses the degree of (dis)agreement in the data \citep{Boonen_et_al_2021, Shannon_1948}. In that sense, the method tries to infer intelligibility from the extent in which a set of transcribers, can identify the words contained in multiple utterances \cite{Boonen_et_al_2021}. 

As the literature suggests, objective rating procedures produce more valid\footnote{validity is understood as the extent to which scores are appropriate for their intended interpretation and use \citep{Lesterhuis_2018, Trochim_2022}.} and reliable\footnote{reliability is though as the extend to which a measure would give us the same result over and over again \citep{Trochim_2022}, i.e. measure something, free from error, in a consistent way.} scores than any other available procedure \citep{Boonen_et_al_2021, Faes_et_al_2021}, as the method does not hinge in the use or production of a \textit{subjective rating scale}, i.e. a scale based on a personal perception of the child's intelligibility. Moreover, the previous advantages are further emphasized by the use of stimuli gathered from spontaneous speech tasks, as they have a greater level of ecological validity, especially compared to contextualized utterances or reading at loud tasks \cite{Flipsen_2006, Ertmer_2011}.

\begin{comment}
	
	The literature suggest two perspectives from which \textit{speech intelligibility} can be assessed: the message and listener's perspective \citep{Boonen_et_al_2020, Boonen_et_al_2021}. The first, also known as acoustic studies, is focused on assessing separately particular characteristics of the speech samples, e.g. their pitch, duration or stress (supra segmental characteristics), or the articulation of vowels and consonants (segmental characteristics) \citep{Rowe_et_al_2018}. Whereas the second, also known as perceptual studies, is centered on making holistic assessments of the speech stimuli, e.g. measure their perceived quality \citep{Boonen_et_al_2020, Boonen_et_al_2021}. On both instances, the stimuli (children's utterances) can be generated from reading at loud, contextualized utterances, or spontaneous speech tasks\footnote{ordered on increasing level of ecological validity \cite{Flipsen_2006, Ertmer_2011}}.
	
	%%%%%%%%%%%%%%%%
	Based on their description, it seems that perceptual are more subjective than acoustic studies, as they do not rely on "objective" measurements, i.e. time duration, wave amplitude, among others, available in the former. However, for the case of SI, there are objective and subjective assessment methodologies.
	%%%%%%%%%%%%%%%%
	
	Furthermore, perceptual studies can use multiple approaches to measure intelligibility, but they can be largely grouped into two: objective and subjective ratings \citep{Hustad_et_al_2020}. In \textit{objective rating} methods, listeners transcribe the children's utterances orthographically or phonetically, and use such information to construct a score. In that sense, in the transcription task, intelligibility can be inferred from the extent a set of transcribers can identify the word contained in an utterance \cite{Boonen_et_al_2021}. In contrast, under \textit{subjective rating} methods, listeners directly infer the utterance's intelligibility score through specific procedures, e.g. absolute holistic, analytic, or comparative judgments, among others. 
	
	It is easy to deduce that \textit{objective rating} methods produce more valid\footnote{the extent to which scores are appropriate for their intended interpretation and use \citep{Lesterhuis_2018, Trochim_2022}.} and reliable\footnote{the extend to which a measure would give us the same result over and over again \citep{Trochim_2022}, i.e. measure something, free from error, in a consistent way.} scores than its \textit{subjective} counterpart, and as a result, are usually used as an objective measure of intelligibility \citep{Boonen_et_al_2021, Faes_et_al_2021}.
	
\end{comment}

However, although the literature is clear on the method's benefits to measure \textit{speech intelligibility} \citep{Boonen_et_al_2020, Boonen_et_al_2021, Hustad_et_al_2020}, we notice the statistical approaches used to model such data still face three important issues, and these come to the detriment of the measurement procedure's sophistication. 

First, as previous paragraphs reveal, the intelligibility scores are `complex' in nature, however, such `complexity' is rarely fully considered in the statistical modeling procedure. The problem with the later is that, because the data does not fulfill the typical assumptions, e.g. normality, it might lead us to erroneous conclusions \textcolor{red}{[citation]}. On the one hand, outcomes such as the number of (un)intelligible words are discrete, while the entropy scores are continuous in nature. In addition, there is the consideration that both measures are constraint in specific bounds, i.e. the (normalized) number of (un)intelligible words cannot be negative, while the entropy scores are in the bounds between zero and one. Finally, given the \textit{objective rating} procedure nature, the scores are produced in a clustered manner, i.e. we observe several score measurement per child. Considering all of the above, it is clear the modeling requires some adjustments to account for all of these nuances in the data.

The statistical procedures applied in the literature have always assumed `normality' \citep{Boonen_et_al_2021, Flipsen_et_al_2006, Hustad_et_al_2020}, and some papers even used multilevel modeling to deal with the clustered nature of the data \citep{Boonen_et_al_2021}, however, to the authors knowledge no paper have dealt with all of the data `complexity' at once. This leads us to believe that there are gains by using more sophisticated statistical models, more specifically, by using generalized linear mixed models (GLMM) \citep{Breslow_et_al_1993, Nelder_et_al_1996, Nelder_et_al_1983}.  

\begin{comment}
Second, many factors are though to influence intelligibility, but the framework in which they are analyzed does not allow to make causal hypothesis. Moreover, it might suffer from problems like including variables that are multicollinear
- hypothesis seem clear cut with classical statistics, but bayesian modeling allow us to have a more nuanced view of the models, their inferences and conclusions

Third, in lieu with the previous no attempt on constructing an actual intelligibility score is made, and therefore, the modeling procedure is applied on surrogate measures, like entropy, but not directly on what it is intended to be measure. For instance, \citet{Flipsen_2006} uses 

From the previous, we believe the field can further benefit from using much more sophisticated statistical analysis tools, than the ones currently used. 
\end{comment}


\begin{comment}
We believe this paper make three specific contributions to the understanding of the factors that drive the intelligibility of spoken language. First, we develop a novel analysis using a latent variable approach \cite{Everitt_1984}. More specifically, we model \textit{speech intelligibility} as a latent variable that can be inferred from the entropy replicates. This method offers three specific benefits. On the one hand, the method `constructs' an intelligibility score, which in turn, allow us to test different hypothesis and even make individual comparisons at the appropriate level. On the other hand, it allow us to control for different sources of variation. This is particularly important as, by failing to account for the appropriate hierarchies in the data, we could be `manufacturing' false confidence in the parameter estimates, leading us to incorrect inferences \cite{McElreath_2020}. Finally, the method also provides a `criterion' on how reliable are the entropy replicates to measure speech intelligibility.

Second, we use Directed Acyclic Graph (DAG) \cite{Pearl_2009, Cinelli_et_al_2021} to depict all the relevant variables though to influence speech intelligibility. We describe in detail our causal and non-causal hypothesis, and supplement our description with a causal diagram. The benefit of the method lies, not only, in that it makes the assumptions of our hypothesis more transparent, but also allow us to derive statistical procedures from the aforementioned causal assumptions \cite{McElreath_2020, Yarkoni_2020, Rohrer_et_al_2021}.

Accompanying the intelligibility assessment methods, the literature supply a myriad of factors that are thought also contribute to the (under)development of intelligible spoken language \cite{Niparko_et_al_2010, Boons_et_al_2012, Gillis_2018, Fagan_et_al_2020}. Among these are audiology related factors, such chronological age, age at implantation, the duration of device use, \textit{hearing age}, bilateral or contralateral cochlear implantation, and the children's preoperative and postoperative hearing levels. On the other hand, there are also child related factors, such as the cause of the hearing impairment (genetic, infections), additional disabilities (mental retardation, speech motor problems), and gender. Finally, there are also environmental factors, such as communication modality. 


Third and final, we wrap the analysis procedure under the Bayesian framework, providing the assumptions, and the steps required to reproduce the computational implementation of the models.



Considering all of the above, this paper seeks to investigates the speech intelligibility levels of normal hearing (NH) versus hearing-impaired children with cochlear implants (HI/CI). For that purpose, ten utterances recordings, from thirty two NH and HI/CI children, were selected from a large corpus of \textit{spontaneously spoken speech} collected by the CLiPS research center. Additionally, we set up an experiment, where one hundred language students transcribed each stimuli to the Qualtrics environment \cite{Qualtrics_2005}. Finally, the transcriptions were transformed into an entropy measure per utterance, which served as our outcome variable.
\end{comment}

%
%
