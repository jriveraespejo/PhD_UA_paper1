%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{S:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Intelligible speech can be defined as the extent in which the elements in a speaker's acoustic signal, e.g. phonemes or words, can be correctly recovered by a listener \cite{Freeman_et_al_2017, Kent_et_al_1989, vanHeuven_2008, Whitehill_et_al_2004}. Intelligible spoken language carries an important societal value, as its attainment requires all core components of speech perception, cognitive processing, linguistic knowledge, and articulation to be mastered \cite{Freeman_et_al_2017}. In that sense, \textit{speech intelligibility} is considered a milestone in children's language development, and more practically, it is qualified as the ultimate checkpoint for the success of speech therapy, and the `gold standard' for assessing the benefit of cochlear implantation \cite{Chin_et_al_2012}. 

Multiple approaches can be taken to quantify \textit{speech intelligibility} \cite{Boonen_et_al_2020, Boonen_et_al_2021, Flipsen_2006, Hustad_et_al_2020}, but among them, \textit{objective rating} methods on stimuli recovered from spontaneous speech tasks have received special attention \cite{Boonen_et_al_2021, Hustad_et_al_2020}. In objective rating methods, listeners transcribe children's utterances orthographically (or phonetically), and use such information to construct an intelligibility score, e.g. reporting the number of (un)intelligible syllables or words in the utterances \cite{Flipsen_2006, Lagerberg_et_al_2014}, or calculating an entropy score, a measure that expresses the degree of (dis)agreement in the transcriptions \cite{Boonen_et_al_2021, Shannon_1948}. In that sense, the method tries to infer intelligibility from the extent in which a set of transcribers, can identify the words contained in the utterances \cite{Boonen_et_al_2021}. 

As the literature suggests, objective rating procedures produce more valid\footnote{validity is understood as the extent to which scores are appropriate for their intended interpretation and use \cite{Lesterhuis_2018, Trochim_2022}.} and reliable\footnote{reliability is though as the extend to which a measure would give us the same result over and over again \cite{Trochim_2022}, i.e. measure something, free from error, in a consistent way.} scores than any other available procedure \cite{Boonen_et_al_2021, Faes_et_al_2021}, as the method does not hinge in the use or production of a \textit{subjective rating scale}, i.e. a scale based on a personal perception of the child's intelligibility. Moreover, the previous advantages are further emphasized by the use of stimuli gathered from spontaneous speech tasks, as they have a greater level of ecological validity, especially compared to contextualized utterances or reading at loud tasks \cite{Flipsen_2006, Ertmer_2011}.

\begin{comment}
	
	The literature suggest two perspectives from which \textit{speech intelligibility} can be assessed: the message and listener's perspective \citep{Boonen_et_al_2020, Boonen_et_al_2021}. The first, also known as acoustic studies, is focused on assessing separately particular characteristics of the speech samples, e.g. their pitch, duration or stress (supra segmental characteristics), or the articulation of vowels and consonants (segmental characteristics) \citep{Rowe_et_al_2018}. Whereas the second, also known as perceptual studies, is centered on making holistic assessments of the speech stimuli, e.g. measure their perceived quality \citep{Boonen_et_al_2020, Boonen_et_al_2021}. On both instances, the stimuli (children's utterances) can be generated from reading at loud, contextualized utterances, or spontaneous speech tasks\footnote{ordered on increasing level of ecological validity \cite{Flipsen_2006, Ertmer_2011}}.
	
	%%%%%%%%%%%%%%%%
	Based on their description, it seems that perceptual are more subjective than acoustic studies, as they do not rely on "objective" measurements, i.e. time duration, wave amplitude, among others, available in the former. However, for the case of SI, there are objective and subjective assessment methodologies.
	%%%%%%%%%%%%%%%%
	
	Furthermore, perceptual studies can use multiple approaches to measure intelligibility, but they can be largely grouped into two: objective and subjective ratings \citep{Hustad_et_al_2020}. In \textit{objective rating} methods, listeners transcribe the children's utterances orthographically or phonetically, and use such information to construct a score. In that sense, in the transcription task, intelligibility can be inferred from the extent a set of transcribers can identify the word contained in an utterance \cite{Boonen_et_al_2021}. In contrast, under \textit{subjective rating} methods, listeners directly infer the utterance's intelligibility score through specific procedures, e.g. absolute holistic, analytic, or comparative judgments, among others. 
	
	It is easy to deduce that \textit{objective rating} methods produce more valid\footnote{the extent to which scores are appropriate for their intended interpretation and use \citep{Lesterhuis_2018, Trochim_2022}.} and reliable\footnote{the extend to which a measure would give us the same result over and over again \citep{Trochim_2022}, i.e. measure something, free from error, in a consistent way.} scores than its \textit{subjective} counterpart, and as a result, are usually used as an objective measure of intelligibility \citep{Boonen_et_al_2021, Faes_et_al_2021}.
	
\end{comment}

However, although the literature is clear on the method's benefits to quantify (measure) \textit{speech intelligibility} \cite{Boonen_et_al_2020, Boonen_et_al_2021, Hustad_et_al_2020}, we notice the statistical approaches used to model such data still face three important issues, and these come to the detriment of the measurement procedure's sophistication. 

First, as previous paragraphs reveal, the intelligibility scores are `complex' in nature, however, such `complexity' is rarely fully considered in the statistical modeling procedure. The problem with the later is that, because the data does not fulfill the typical assumptions, e.g. normality, its analysis under such models might lead us to erroneous conclusions \textcolor{red}{[citation]}. On the one hand, outcomes such as the number of (un)intelligible words are discrete, while the entropy scores are continuous in nature. In addition, there is the consideration that both scores are constraint in specific bounds, i.e. the number of (un)intelligible words cannot be negative, while the entropy scores are in the bounds between zero and one. Finally, given the rating procedure's nature, the scores are produced in a clustered manner, i.e. we observe several score measurements per child. 

So far the literature shows that, even when the data does not conform to the `normality' assumption, the applied statistical procedures are still supported on it, examples of this can be seen in \citet{Boonen_et_al_2021, Flipsen_et_al_2006} and \citet{Hustad_et_al_2020}. In addition, some papers in the literature have even used (hierarchical) multilevel modeling to deal with the clustered nature of the data, e.g. \citet{Boonen_et_al_2021}. However, to the authors knowledge, no paper have dealt with all of the data nuances at once, which leads us to believe that, by using more sophisticated statistical models we could improve our statistical inferences. 

Second, although the literature suggest the number of (un)intelligible words or the entropy of transcriptions are scores that capture the level of intelligibility in a child, it is easy to notice these two can still be considered surrogate measures of it, i.e. scores that indirectly reflect what is intended to be measured. The latter is important because it implies these outcomes are `measured with error', resulting from considering that there is an unobserved (latent) `construct' that is responsible for the observed scores variation, i.e. the \textit{speech intelligibility}. Moreover, it is important to recognize that this `measurement error' is of a different kind that the one produced by the clustered nature of the data, and that again, by failing to account for it, we would be led to incorrect inferences \cite{deHaan_et_al_2019}.

To the authors knowledge, no attempt to create such intelligibility latent 'construct' have been made. Therefore, we believe the literature could benefit from showing how to implement such procedure in a statistical model, in combination with the procedures needed to account for the other nuances in the data.

Third, even though the literature supplies a myriad of factors that are thought to contribute to the (under)development of intelligible spoken language \cite{Boons_et_al_2012, Gillis_2018, Fagan_et_al_2020, Niparko_et_al_2010}, no transparent framework of analysis is used to determine which factors are relevant, or conforms to valid and actionable causal hypothesis. The lack of such framework not only makes the selection and assessment of relevant factors harder, but also hinders the researcher's ability to avoid facing some common statistical issues related to such selection, e.g. determine which factors can be analyzed in tandem without facing collinearity problems, which ultimately affects our inference capabilities \cite{Farrar_et_al_1967}.

As it was suggested, several factors are proposed by the literature, but these can be largely grouped into three categories: audiology, child and environmental related factors. For the first, they are the chronological age, age at implantation, the duration of device use, `hearing' age, bilateral or contralateral cochlear implantation, and the children's preoperative and postoperative hearing levels. For the second, there is the etiology or the cause of the hearing impairment (e.g. genetic, infections), additional disabilities (e.g. mental retardation, speech motor problems), and gender. Finally for the last, there is the communication modality. 

Therefore, considering the aforementioned variables, and the relation complexity with themselves and the outcome, we believe that a causal framework would allow us to integrate previous literature on the matter, and also provide a more transparent way of state and analyze our research hypothesis.

Considering all of the above, we believe this paper make four specific contributions to the field. First, we develop a novel analysis using a Generalized Linear Latent and Mixed Model (GLLAMM) \cite{Rabe_et_al_2004a, Rabe_et_al_2004b, Rabe_et_al_2004c, Rabe_et_al_2012, Skrondal_et_al_2004a}. More specifically, we model \textit{speech intelligibility} as a latent variable \cite{Everitt_1984}, that can be inferred from the repeated entropy scores, where the latter is then modeled under a Generalized Linear Mixed Model (GLMM) \cite{Breslow_et_al_1993, Nelder_et_al_1996, Nelder_et_al_1983}. 

The previous statistical method offers three specific benefits. On the one hand, it allow us to consider all of the data nuances at once, i.e. we can model our `non normal' data, and control for the different sources of variation (error) observed in it. The latter is particularly important because, as it was mentioned, by failing to account for these sources we could be `manufacturing' false confidence in the parameter estimates, leading us to incorrect inferences \cite{McElreath_2020}. On the other hand, the method provides a way to `construct' an intelligibility scale. This in turn, allow us to test our research hypotheses on the measure of interest, and even make individual comparisons at the children level. Finally, resulting from the statistical procedure sophistication, the method also provides a `criterion' on how reliable the repeated entropy measures are to quantify speech intelligibility.

Second, we use Directed Acyclic Graph (DAG) \cite{Pearl_2009, Cinelli_et_al_2021} to depict all the relevant variables though to influence \textit{speech intelligibility}. We describe in detail our causal and non-causal hypothesis, and supplement our description with a causal diagram. The benefit of the method lies, not only, in that it makes the assumptions of our hypothesis more transparent, but also allow us to derive statistical procedures from our causal assumptions \cite{McElreath_2020, Yarkoni_2020, Rohrer_et_al_2021}.

Third, given the complexity of the statistical procedure, we wrap the analysis under the Bayesian framework, providing the assumptions and steps required to reproduce the computational implementation of the models. The general reasons for using Bayesian statistics in our research are that the framework can handle all kinds of data-generating processes \cite{Fox_2010}, and it lends itself easily to complex and over-parameterized models \cite{Baker_1998, Kim_1999}, characteristics that define our implementation. Furthermore, although the framework have similar estimation capabilities as its frequentist counterpart \cite{Baker_1998, Hsieh_2010, Wollack_2002}, some specific scenarios in our current research also favors its use, i.e. the need of inferences with a small sample size \cite{Fox_2010, McElreath_2020, Skrondal_et_al_2004a}, and the need of confining some parameters in a permitted space \cite{Martin_et_al_1975}, e.g. variances confined to positive values. Moreover, since the main output of Bayesian statistics are not point estimates, but rather the posterior distribution of the parameters' possible values \cite{McElreath_2020}, the framework allow us to have a more nuanced view of our inferences and conclusions.

Fourth, we implement all of the above in a data set consisting of repeated entropy measures, with the purpose of determine which factors affect the \textit{speech intelligibility} levels of normal hearing (NH) versus hearing-impaired children with cochlear implants (HI/CI). The entropy measures were calculated using the transcriptions of one hundred language students from the University of Antwerp, where each student transcribed the stimuli to the Qualtrics environment \cite{Qualtrics_2005}. The stimuli consisted in ten utterances recordings for each of the thirty two NH and HI/CI children, selected from a large corpus of \textit{spontaneously spoken speech} collected by the Computational Linguistic and Psycholinguistics Research Centre (CLiPS).

Ultimately, we find the proposed methods bring new insights about the use of repeated entropy scores to measure \textit{speech intelligibility}, as much as, new insights on how the factors affect the (under)development of children's intelligibility.
%
%
