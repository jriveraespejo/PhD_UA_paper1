%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{S:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Why intelligibility is important?
Intelligible spoken language requires all core components of speech perception, cognitive processing, linguistic knowledge, and articulation to be mastered \citep{Freeman_et_al_2017}. In that sense, its attainment carries an important societal value, as it is considered a milestone in children's language development; and more practically, it is qualified as the ultimate checkpoint for the success of speech therapy, and the "gold standard" for assessing the benefits of cochlear implantation \citep{Chin_et_al_2012}.

% What is intelligibility?
But what is speech intelligibility?. Intelligibility is usually conceptualized as the extent to which the elements in an acoustic signal generated by a speaker, e.g. phonemes or words, can be correctly recovered by a listener \citep{Freeman_et_al_2017, Kent_et_al_1989, Munro_et_al_1999, vanHeuven_2008, Whitehill_et_al_2004}. The latter definition sets a clear contrast with comprehensibility, which involves the listener’s ability to understand the sounds' message and its intent \citep{Munro_et_al_1999, Smith_et_al_1985}.

% why is a complicated concept to "grab"?
But the literature reveals that intelligibility is an intricate concept, with particular challenges to its assessment/measurement. The latter is because intelligibility can be affected by features of the communicative environment, such as noise \citep{Munro_1998}; by features of the speaker, like speaking rate \citep{Munro_et_al_1998} or accent \citep{Jenkins_2000, Ockey_et_al_2016}}; or features of the listener, like vocabulary mastery \citep{Varonis_et_al_1985}. Moreover, this further emphasizes another aspect of the concept: its dynamic nature, where changes in intelligibility stem from the speaker's online adaptations to the listener and/or context.

% How can be measured/assessed?
Therefore, the literature suggests there are three aspects to the study of speech sounds, and therefore, three from which intelligibility can be asssessed: the acoustic, articulatory, and auditory aspects \citep{Gudivada_et_al_2018}. The first is focused on assessing the transmission and physical properties of speech sounds \citep{Boonen_et_al_2020, Boonen_et_al_2021}. The second is more concerned with the sounds' production \citep{Rowe_et_al_2018}. While the last, center its attention on the speech sounds' perception, i.e. how the stimuli are perceived by a listener \citep{Boonen_et_al_2020, Boonen_et_al_2021}.

\begin{comment}	
	
	Based on their description, it seems that perceptual are more subjective than acoustic studies, as they do not rely on "objective" measurements, i.e. time duration, wave amplitude, among others, available in the former. However, for the case of SI, there are objective and subjective assessment methodologies.
		
\end{comment}

% How do you do measure it in perceptual studies?
Focusing our attention on the last one, perceptual studies also use multiple approaches to measure intelligibility, but they can be largely grouped into two: subjective and objective ratings methods \citep{Hustad_et_al_2020}. In the former, listeners directly infer the intelligibility score of the speech samples through different procedures. While in the latter, listeners transcribe children's utterances orthographically (or phonetically), and use these as information to construct an entropy score that expresses the degree of (dis)agreement in the transcriptions \citep{Boonen_et_al_2021, Shannon_1948}.

% What you obtain from the methods?
Consequently, objective ratings try to infer intelligibility from the extent to which a set of transcribers can identify the words contained in the utterances \citep{Boonen_et_al_2021}. While subjective ratings, try to directly produce a score based on a listener's perception of the sounds' intelligibility. In either case, the methods produce a proxy measure of the speaker's intelligibility as judged by a listener, a snapshot of his/her performance under a specific set of circumstances \citep{Hustad_et_al_2020}. 

% Why they are a valid procedure?
Moreover, the methods' validity, i.e. the extent to which scores are appropriate for their intended interpretation and use \citep{Lesterhuis_2018, Trochim_2022}, is founded on the idea that intelligibility is an intuitively understood notion, "something" that anyone can judge, but that can only be measured indirectly because of its entanglement with other features of the communication \citep{Guilford_1954, Stevens_1946}.

\begin{comment}
	
	As the literature suggests, objective rating procedures produce more valid\footnote{validity is understood as the extent to which scores are appropriate for their intended interpretation and use \citep{Lesterhuis_2018, Trochim_2022}.} and reliable\footnote{reliability is though as the extend to which a measure would give us the same result over and over again \citep{Trochim_2022}, i.e. measure something, free from error, in a consistent way.} scores than any other available procedure \citep{Boonen_et_al_2021, Faes_et_al_2021}, as the method does not hinge in the use or production of a \textit{subjective rating scale}, i.e. a scale based on a personal perception of the child's intelligibility.
	
\end{comment}

% where our main interest lies?
Recently in the literature, objective rating procedures applied on children's utterances recovered from spontaneous speech tasks have received special attention \citep{Boonen_et_al_2021, Hustad_et_al_2020}. The scores produced from these tasks are characterized by their clustered and bounded nature. The former happens because the data register multiple measurements per child; more specifically, one score per utterance, where multiple utterances are assessed. While the latter happens because the entropy score values are expresed in the continuum between zero and one \citep{Shannon_1948}.

\begin{comment}	
	
	Moreover, the previous advantages are further emphasized by the use of stimuli gathered from spontaneous speech tasks, as they have a greater level of ecological validity, especially compared to contextualized utterances or reading at loud tasks \citep{Flipsen_2006, Ertmer_2011}.
	
\end{comment}

% The literature has been doing progress, but is not enough
Although the literature has been clear on the aforementioned method benefits to (indirectly) quantify intelligibility \citep{Boonen_et_al_2020, Boonen_et_al_2021, Hustad_et_al_2020}, we notice the statistical procedures used to model such data have not been fully at par to the measurement procedure's sophistication.

% they have dealt with the clustering, and why it was a good idea
First, previous research have dealt with the data clustering, but ignored its bounded nature. More specifically, \cite{Boonen_et_al_2021} modeled and tested some research hypotheses of interest on similar data, through the use of multilevel linear models (MLM). To understand the importance of this decision, it is relevant to highlight why the use of such models is a requirement when the data is clustered. 

When more than one observation arises from the same individual, location, or time, then traditional (single-level) statistical models may mislead us \citep{McElreath_2020}. The reason for this, is that one of the main assumptions of these models gets violated: the independence of errors \citep{Finch_et_al_2019}.

The latter is easier to understand with a thought experiment. Consider the scenario hinted in previous paragraphs: we observe one entropy score per utterance, for a total of ten utterances per child. In this scenario, it would be reasonable to believe the ten entropy scores observed for the same child would be more similar with one another, than what they are, with the scores observed for other children. This within-child correlation would be due, for example, to having the same speech pattern,  articulation, linguistic knowledge, among other reasons, perceived by the listener.

The presence and non-recognition of this within-child correlation in the data will, in turn, result in two know statistical issues \citep{Finch_et_al_2019}. On the one hand, the inapropriate estimation of the standard error parameters of the model. This is important, as biased parameters might lead us to less appropriate statistical inferences, e.g. smaller standard error with larger t-statistics and smaller p-values, that lead to the rejection of a "true" null hypothesis (Type I error). On the other hand, if the multilevel structure of the data is ignored, we may miss important relationships involving each level in the data. Considering our thought experiment, it is easy to see the presence of two levels in our data: utterances (level 1) and children (level 2). Notice that we might have different information (variables) at each level that explains the data behavior, and by not appropriately including them, we will be suggesting the use of an incorrect model for understanding the outcome of interest.

% the previous is good but not enough
Therefore, it is clear why modeling the data clustering is important from the statistical point of view. However, we argue that the latter practice is not sufficient, because it has not considered the bounded nature of the data, which might lead to a different set of statistical problems.

To understand the preceding statement, it is important to first highlight the main assumption of MLMs: normally distibuted errors \citep{Nelder_et_al_1983}. But what the normality assumption implies?. First, it imply that we are assuming that our outcome of interest is, by extension, also normally distributed, i.e. the entropy scores and any transformation of them (e.g. its average) can take any value in the real line without constraint. And second, it implies that we are mainly interested on modeling the outcome's location (average), where the estimation of its spread (variance) takes a secondary role, only justified by the need of appropriate inferences.

It is clear from the entropy scores description, that the first implication of assuming normally distributed outcomes is not fulfilled. This is a problem because it implies that we might be allowing the final multilevel model to test hypotheses and produce predictions based on a modeling assumption that do not match the data reality, e.g. the model might produce negative entropy values \citep{McElreath_2020}.

Moreover, a simple thought experiment can show us why ignoring the second implication can also mislead us. Consider children with three different patterns for ten entropy scores, all reporting the same entropy mean of $0.5$. The patterns are: (a) scores closely agglomerated around $0.5$, (b) scores loosely aglomerated around $0.5$, and finally, (c) half of the scores agglomerated around $0.1$ and the other half around $0.9$. From the mean score we can say that the three children have an "average" level of intelligibility. However, from the spread of the scores we can notice that more uncertainty (to the asesssment of "average") should be assigned to child (c), followed by (b) and finally (a). This just mean that we can be more confident that child (a) has an "average" level of intelligibility, than in the other two cases, where (c) represent one extreme example of uncertainty. 

In this sense, we notice that the need to model the distribution spread is no longer justified only by a demand of appropriate inferences, but also because it informs about the individual's intelligibility. Therefore, it is clear that more sophisticated statistical procedures are needed to model our data.

\begin{comment}
	
	First, as previous paragraphs reveal, the intelligibility scores are `complex' in nature, however, such `complexity' is rarely fully considered in the statistical modeling procedure. The problem with the later is that, because the data does not fulfill the typical assumptions, e.g. normality, its analysis under such models might lead us to erroneous conclusions \textcolor{red}{[citation]}. On the one hand, outcomes such as the number of (un)intelligible words are discrete, while the entropy scores are continuous in nature. In addition, there is the consideration that both scores are constraint in specific bounds, i.e. the number of (un)intelligible words cannot be negative, while the entropy scores are in the bounds between zero and one. Finally, given the rating procedure's nature, the scores are produced in a clustered manner, i.e. we observe several score measurements per child. 
	
	So far the literature shows that, even when the data does not conform to the `normality' assumption, the applied statistical procedures are still supported on it, examples of this can be seen in \citep{Boonen_et_al_2021, Flipsen_et_al_2006} and \citep{Hustad_et_al_2020}. In addition, some papers in the literature have even used (hierarchical) multilevel modeling to deal with the clustered nature of the data, e.g. \citep{Boonen_et_al_2021}. However, to the authors knowledge, no paper have dealt with all of the data nuances at once, which leads us to believe that, by using more sophisticated statistical models we could improve our statistical inferences. 

\end{comment}

Second, although the literature suggest the entropy scores "captures" intelligibility, we can say that these scores are a coarse version of it, i.e. an uncertain manifestation of a child's intelligibility. Stated in other words, there is an unobserved (latent) intelligibility construct that is responsible for what it is observed on the entropy scores and their variation. Notice the previous can be stated because of two reasons. First, we observe multiple entropy scores per child. Second, the scores are entropy not "intelligibility" scores. 

Therefore, if we hope to understand or intervene on the factors that drives speech intelligibility, first one needs to "construct" an intelligibility scale from the data \citep{Carroll_2006}, allowing us to test our research hypotheses at the appropriate (children) level. Furthermore, the literature emphasize that failing to model this phenomena as a "latent construct" would also lead us to incorrect inferences \citep{deHaan_et_al_2019}.

\begin{comment}
			
	If you take a look at the research most of the time these phenomena are not really modelled as characteristics of persons, but as characteristics of smaller units of observation from that person (e.g. utterances; trials; …)
	
	Not approaching these phenomena as a ‘latent construct’ has statistical consequences (this is the crux of the problem; the statistical but also a conceptual problem)
	
	Second, although the literature suggest the number of (un)intelligible words or the entropy of transcriptions are scores that capture the level of intelligibility in a child, it is easy to notice these two can still be considered surrogate measures of it, i.e. scores that indirectly reflect what is intended to be measured. The latter is important because it implies these outcomes are `measured with error', resulting from considering that there is an unobserved (latent) `construct' that is responsible for the observed scores variation, i.e. the \textit{speech intelligibility}. Moreover, it is important to recognize that this `measurement error' is of a different kind that the one produced by the clustered nature of the data, and that again, by failing to account for it, we would be led to incorrect inferences \citep{deHaan_et_al_2019}.
	
	To the authors knowledge, no attempt to create such intelligibility latent 'construct' have been made. Therefore, we believe the literature could benefit from showing how to implement such procedure in a statistical model, in combination with the procedures needed to account for the other nuances in the data. 
	
	Third, even though the literature supplies a myriad of factors that are thought to contribute to the (under)development of intelligible spoken language \citep{Boons_et_al_2012, Gillis_2018, Fagan_et_al_2020, Niparko_et_al_2010}, no transparent framework of analysis is used to determine which factors are relevant, or conforms to valid and actionable causal hypothesis. The lack of such framework not only makes the selection and assessment of relevant factors harder, but also hinders the researcher's ability to avoid facing some common statistical issues related to such selection, e.g. determine which factors can be analyzed in tandem without facing collinearity problems, which ultimately affects our inference capabilities \citep{Farrar_et_al_1967}.
	
	As it was suggested, several factors are proposed by the literature, but these can be largely grouped into three categories: audiology, child and environmental related factors. For the first, they are the chronological age, age at implantation, the duration of device use, `hearing' age, bilateral or contralateral cochlear implantation, and the children's preoperative and postoperative hearing levels. For the second, there is the etiology or the cause of the hearing impairment (e.g. genetic, infections), additional disabilities (e.g. mental retardation, speech motor problems), and gender. Finally for the last, there is the communication modality. 
	
	Therefore, considering the aforementioned variables, and the relation complexity with themselves and the outcome, we believe that a causal framework would allow us to integrate previous literature on the matter, and also provide a more transparent way of state and analyze our research hypothesis.
	
\end{comment}

In that sense, the aim of this research is to propose a novel analysis of the entropy data using a Bayesian implementation of the Generalized Linear Latent and Mixed Model (GLLAMM) \citep{Rabe_et_al_2004a, Rabe_et_al_2004b, Rabe_et_al_2004c, Rabe_et_al_2012, Skrondal_et_al_2004a}. The statistical procedure offers four benefits. First, it allows to appropriately model the bounded nature of the entropy data. Second, it provides a way to "construct" the speaker's latent intelligibility scale. Third, it allow us to test our research hypothesis at the appropriate level. And fourth, as a result from the first two, we successfully avoid producing false confidence in the parameter estimates, which help us to produce better informed statistical inferences \citep{McElreath_2020}.

\begin{comment}
	
	The previous statistical method offers three specific benefits. On the one hand, it allow us to consider all of the data nuances at once, i.e. we can model our `non normal' data, and control for the different sources of variation (error) observed in it. The latter is particularly important because, as it was mentioned, by failing to account for these sources we could be `manufacturing' false confidence in the parameter estimates, leading us to incorrect inferences \citep{McElreath_2020}. On the other hand, the method provides a way to `construct' an intelligibility scale. This in turn, allow us to test our research hypotheses on the measure of interest, and even make individual comparisons at the children level. Finally, resulting from the statistical procedure sophistication, the method also provides a `criterion' on how reliable the repeated entropy measures are to quantify speech intelligibility.
	
	Second, we use Directed Acyclic Graph (DAG) \citep{Pearl_2009, Cinelli_et_al_2021} to depict all the relevant variables though to influence \textit{speech intelligibility}. We describe in detail our causal and non-causal hypothesis, and supplement our description with a causal diagram. The benefit of the method lies, not only, in that it makes the assumptions of our hypothesis more transparent, but also allow us to derive statistical procedures from our causal assumptions \citep{McElreath_2020, Yarkoni_2020, Rohrer_et_al_2021}.
	
	Third, given the complexity of the statistical procedure, we wrap the analysis under the Bayesian framework, providing the assumptions and steps required to reproduce the computational implementation of the models. The general reasons for using Bayesian statistics in our research are that the framework can handle all kinds of data-generating processes \citep{Fox_2010}, and it lends itself easily to complex and over-parameterized models \citep{Baker_1998, Kim_1999}, characteristics that define our implementation. Furthermore, although the framework have similar estimation capabilities as its frequentist counterpart \citep{Baker_1998, Hsieh_2010, Wollack_2002}, some specific scenarios in our current research also favors its use, i.e. the need of inferences with a small sample size \citep{Fox_2010, McElreath_2020, Skrondal_et_al_2004a}, and the need of confining some parameters in a permitted space \citep{Martin_et_al_1975}, e.g. variances confined to positive values. Moreover, since the main output of Bayesian statistics are not point estimates, but rather the posterior distribution of the parameters' possible values \citep{McElreath_2020}, the framework allow us to have a more nuanced view of our inferences and conclusions.

\end{comment}
	
We find that when the proposed method is used to investigate the speech intelligibility levels of normal hearing (NH) versus hearing-impaired children with cochlear implants (HI/CI), in a data composed of ten utterances recordings from thirty two NH and HI/CI children selected from a large corpus of spontaneously spoken speech collected by the CLiPS research center, it brings bring new insights about the use of replicated entropy scores to measure intelligibility. Furthermore, the method also provide a way to assess how some factors affect the (under)development of children's intelligibility.


\begin{comment}
	
	Fourth, we implement all of the above in a data set consisting of repeated entropy measures, with the purpose of determine which factors affect the \textit{speech intelligibility} levels of normal hearing (NH) versus hearing-impaired children with cochlear implants (HI/CI). The entropy measures were calculated using the transcriptions of one hundred language students from the University of Antwerp, where each student transcribed the stimuli to the Qualtrics environment \citep{Qualtrics_2005}. The stimuli consisted in ten utterances recordings for each of the thirty two NH and HI/CI children, selected from a large corpus of \textit{spontaneously spoken speech} collected by the Computational Linguistic and Psycholinguistics Research Centre (CLiPS).
	
	On the one hand, the method reveal that, not integrating the bounded nature of the data in the modeling procedure could lead us to wrongful statistical conclusions. More precisely, it could lead us to an overestimation of the parameter estimates' precision.
	
	Lastly, our hypothesis tests reveal that hearing impaired children with cochlear implants (HI/CI) and genetic etiology have similar levels of intelligibility as normal hearing kids (NH), when both groups have a `hearing ages' of five. However, the same cannot be said for children with other etiologies, like CMV infection or other causes, as they start a significantly lower level of intelligibility at same `ages'. Moreover, our tests found enough evidence to assert that NH children develop their intelligibility with each `hearing year' at a higher rate than HI/CI kids. This offer evidence contrary to what was previously found \citep{Boonen_et_al_2021}.
	
	Finally, we observe our results support the hypothesis that HI/CI children with severe hearing loss, as accounted by the pure tone average, develop their language at a slower rate than their NH counterparts.

\end{comment}
%
%
