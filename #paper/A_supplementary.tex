%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supplementary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%###############################
\subsection{Children characteristics} \label{sSA:characteristics}
%###############################
%
Table \ref{tab:characteristics} shows the detailed information of the sampled children. The referred table includes the variable used for the matching procedure, i.e. chronological age, while also additional variables thought to be relevant for our hypothesis. No other variables are included as no known additional comorbidities, beside their hearing impairment, are suspected.
%
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|ccccccc|} 
		\hline
		& Gender & Chronological & Device length & Hearing & Etiology & \multicolumn{2}{c |}{PTA (dB.)} \\[0.5ex]
		\cline{7-8}
		Child & & age (y;m) & of use (y;m) & age (y;m) & & unaided & aided \\[0.5ex] 
		\hline\hline
		& \multicolumn{7}{l |}{ \textbf{HI/CI children} } \\
		\rowcolor{gray}
		1 & female & 05;07 & 05;00 & 05;00 & Genetic & 120 & 19 \\
		2 & male & 06;04 & 05;09 & 05;09 & CMV & 106 & 23 \\
		\rowcolor{gray}
		3 & male & 06;07 & 05;10 & 05;10 & Genetic & 114 & 35 \\
		4 & female & 06;10 & 06;00 & 06;00 & Unknown & 120 & 20 \\ 
		\rowcolor{gray}
		5 & female & 07;00 & 06;03 & 06;03 & CMV & 115 & 25 \\ 
		6 & male & 07;00 & 05;08 & 05;08 & Genetic & 93 & 32 \\
		\rowcolor{gray}
		7 & female & 07;00 & 06;08 & 06;08 & Genetic & 117 & 17 \\
		8 & female & 07;00 & 05;05 & 05;05 & Unknown & 112 & 42 \\
		\rowcolor{gray}
		9 & male & 07;00 & 05;05 & 05;05 & CMV & 120 & 15 \\ 
		10 & female & 07;01 & 05;11 & 05;11 & Genetic & 120 & 35 \\		
		\rowcolor{gray}
		11 & male & 07;01 & 05;07 & 05;07 & Genetic & 113 & 42 \\
		12 & male & 07;02 & 06;05 & 06;05 & Genetic & 120 & 37 \\
		\rowcolor{gray}
		13 & male & 07;08 & 06;10 & 06;10 & CMV & 114 & 27 \\
		14 & male & 07;09 & 06;02 & 06;02 & CMV & 120 & 35 \\
		\rowcolor{gray}
		15 & male & 08;07 & 07;10 & 07;10 & CMV & 120 & 33 \\
		16 & male & 08;08 & 09;09 & 09;09 & Genetic & 95 & 27 \\
		
		\hline
		
		& \multicolumn{7}{l |}{ \textbf{NH children} } \\	
		\rowcolor{gray}
		17 & female & 06;05 & n.a. & 06;05 & n.a. & n.a. & n.a.\\
		18 & female & 06;06 & n.a. & 06;06 & n.a. & n.a. & n.a.\\
		\rowcolor{gray}
		19 & female & 06;07 & n.a. & 06;07 & n.a. & n.a. & n.a.\\
		20 & female & 06;09 & n.a. & 06;09 & n.a. & n.a. & n.a.\\
		\rowcolor{gray}
		21 & female & 06;09 & n.a. & 06;09 & n.a. & n.a. & n.a. \\
		22 & male & 06;09 & n.a. & 06;09 & n.a. & n.a. & n.a.\\ 
		\rowcolor{gray}
		23 & male & 06;09 & n.a. & 06;09 & n.a. & n.a. & n.a.\\
		24 & male & 06;10 & n.a. & 06;10 & n.a. & n.a. & n.a.\\
		\rowcolor{gray}
		25 & female & 07;01 & n.a. & 07;01 & n.a. & n.a. & n.a.\\
		26 & male & 07;01 & n.a. & 07;01 & n.a. & n.a. & n.a.\\
		\rowcolor{gray}
		27 & male & 07;04 & n.a. & 07;04 & n.a. & n.a. & n.a.\\
		28 & female & 07;08 & n.a. & 07;08 & n.a. & n.a. & n.a.\\
		\rowcolor{gray}
		29 & male & 07;08 & n.a. & 07;08 & n.a. & n.a. & n.a.\\
		30 & female & 07;09 & n.a. & 07;09 & n.a. & n.a. & n.a.\\ 
		\rowcolor{gray}
		31 & female & 08;00 & n.a. & 08;00 & n.a. & n.a. & n.a.\\
		32 & female & 08;01 & n.a. & 08;01 & n.a. & n.a. & n.a.\\	 
		\hline
		\multicolumn{8}{l}{\footnotesize{(y;m) = (years;months)}} \\
		\multicolumn{8}{l}{\footnotesize{n.a. = not applicable / not available}} \\
	\end{tabular}
	\caption[Characteristics of selected children]{Characteristics of selected children.}
	\label{tab:characteristics}
\end{table}
%
%
%###############################
\newpage
\subsection{Experiment details}
%###############################
%
%-------------------------------
\subsubsection{Transcription task} \label{ssSA:transcription}
%-------------------------------
%
The setting for the transcription task comprised the following steps \citep{Boonen_et_al_2020, Boonen_et_al_2021}:
%
\begin{enumerate} %\itemsep1pt
	\item the listener took a seat in front of a computer screen, located at the campus' computer laboratory.
	%
	\item the listener opened Qualtrics \cite{Qualtrics_2005} and select the transcription task.
	%
	\item the listener read two set of instructions presented on the computer screen about:
	\begin{enumerate}
		\item \textit{how to perform the task}, e.g. the listeners were instructed to write one \texttt{X} to replace an unintelligible word, part of an utterance, or a complete utterance,
		\item \textit{the aspects not to consider for the task}.
	\end{enumerate}
	%
	\item the listener hear the stimuli through high quality headphones, set at a comfortable volume.
	%
	\item the listener wrote the orthographic transcriptions of the utterances, in a free text field in the environment. 
	%
\end{enumerate}
%
%
%-------------------------------
\subsubsection{Entropy calculation} \label{ssSA:entropy} 
%-------------------------------
%
The outcome from the transcription task was obtained following a two step procedure \citep{Boonen_et_al_2021}. First, we aligned the participant's orthographic transcriptions, at the utterance level, in a column-like grid structure similar to the one presented in Table \ref{tab:align_example}. This step was repeated for every one of the $6400$ transcriptions. Lastly, we computed the entropy measure of the aligned transcriptions as in \citet{Shannon_1948}: 
%
\begin{equation} \label{eq:entropy}
	H = H(\pmb{p}) = \frac{-\sum_{i=1}^{n} p_{i} \cdot \log_{2}(p_{i})}{\log_{2}(N)}
\end{equation}
%
where $H$ is bounded in the continuum $[0,1]$, $n$ denotes the number of word occurrences within each utterance, $p_{i}$ the probability of such word occurrence, and $N$ the total number of aligned transcriptions per utterance.
%
\begin{comment}
under DoE literature, the design corresponds to $32$ experimental units with $10$ replicates each, making a total of $320$ experimental runs. Moreover, we register $20$ duplicates (transcriptions) for each run, making a total of $6400$ transcriptions.
\end{comment}
%
\begin{table}[h!]
	\centering
	\begin{tabular}{| c | ccccc | } 
		\hline
		Transcription & \multicolumn{5}{c |}{Utterance} \\ [0.5ex]
		\cline{2-6}
		number & 1 & 2 & 3 & 4 & 5 \\ [0.5ex] 
		\hline\hline
		1 & de & jongen & ziet & een & kikker \\ 
		\rowcolor{gray}
		& the & boy & see & a & frog \\ 
		\hline
		2 & de & jongen & ziet & de & [X] \\
		\rowcolor{gray}
		& the & boy & sees & the & [X] \\ 
		\hline
		3 & de & jongen & zag & [B] & kokkin \\
		\rowcolor{gray}
		& the & boy & saw & [B] & cook \\ 
		\hline
		4 & de & jongen & zag & geen & kikkers \\
		\rowcolor{gray}
		& the & boy & saw & no & frogs \\ 
		\hline
		5 & de & hond & zoekt & een & [X] \\
		\rowcolor{gray}
		& the & dog & searches & a & [X] \\ 
		\hline\hline
		Entropy & $0$ & $0.3109$ & $0.6555$ & $0.8277$ & $1$ \\
		\hline
		\multicolumn{4}{l}{\footnotesize{[B] = blank space, [X] = unidentifiable word}}
	\end{tabular}
	\caption[Alignment and entropy calculation]{Alignment and entropy calculation. Extracted from \citet{Boonen_et_al_2021}, and slightly modified with illustrative purposes.}
	\label{tab:align_example}
\end{table}
%

Entropy was used as a quantification of (dis)agreement between listeners' transcriptions, i.e. utterances yielding a high degree of agreement between transcribers were considered highly intelligible, and therefore registered a lower entropy $\left( H \rightarrow 0 \right)$. In contrast, utterances yielding a low degree of agreement were considered as exhibiting low intelligibility, and therefore registered a higher entropy $\left( H \rightarrow 1 \right)$ \citep{Boonen_et_al_2021, Faes_et_al_2021}. 

To exemplify relevant scenarios for the procedure, we generate the entropy for utterances $2$, $4$ and $5$ present in Table \ref{tab:align_example}. To make the example easy to calculate, we assume our data consisted only of five transcriptions in total ($N=5$).

For the second utterance, we observe that four transcriptions identify it with the word \textit{jongen}, while the last with the word \textit{hond}. Therefore, we registered two word occurrences ($n=2$), with probabilities $\pmb{p} = (p_{1}, p_{2}) = (4/5, 1/5)$, and entropy measure equal to:
%
\begin{align*}
	H &= \frac{-\sum_{i=1}^{2} p_{i} \cdot \log_{2}(p_{i})}{\log_{2}(5)} \\
	%
	&= \frac{- \left[ 0.8 \log_{2}(0.8) + 0.2 \log_{2}(0.2) \right] }{\log_{2}(5)} \\
	%
	&\approx 0.3109
\end{align*} 
%
For the fourth utterance, we observe that two transcriptions identify it with the word \textit{een}, one with \textit{de}, one with \textit{geen}, and one with a blank space [B]. Notice the blank space was not expected in such position, therefore, it was considered as a different word occurrence. As a result, the scenario had four word occurrences ($n=4$), with probabilities $\pmb{p} = (p_{1}, p_{2}, p_{3}, p_{4}) = (2/5, 1/5, 1/5, 1/5)$, and entropy measure equal to:
%
\begin{align*}
	H &= \frac{-\sum_{i=1}^{4} p_{i} \cdot \log_{2}(p_{i})}{\log_{2}(5)} \\
	%
	&= \frac{- \left[ 0.4 \log_{2}(0.4) + 3 \cdot 0.2 \log_{2}(0.2) \right] }{\log_{2}(5)} \\
	%
	&\approx 0.8277
\end{align*} 
%
Finally, for the fifth utterance, we observe that all of the  transcriptions identify it with different words. Notice we consider the unidentifiable word [X] in the second transcription, as being different from the one in the last. This is done to avoid the artificial reduction of the entropy measure, as [X] values already indicate the lack of intelligibility of the word. Therefore, we registered five word occurrences ($n=5$), with probabilities $\pmb{p} = (p_{1}, \dots, p_{5}) = (1/5, \dots, 1/5)$, and entropy measure equal to:
%
\begin{align*}
	H &= \frac{-\sum_{i=1}^{5} p_{i} \cdot \log_{2}(p_{i})}{\log_{2}(5)} \\
	%
	&= \frac{- 5 \cdot 0.2 \log_{2}(0.2) }{\log_{2}(5)} \\
	%
	&= 1
\end{align*} 
%
%
%###############################
\subsection{About speech intelligibility} \label{sSA:SI}
%###############################
%
As described in the introduction, intelligible speech can be defined as the extent to which the elements in an speaker's acoustic signal, e.g. phonemes or words, can be correctly recovered by a listener \citep{Kent_et_al_1989, Whitehill_et_al_2004, vanHeuven_2008, Freeman_et_al_2017}. More specifically, in the context of the transcription task, speech intelligibility can be inferred from the extent a set of transcribers can identify the words contained in an utterance \cite{Boonen_et_al_2021}.

Therefore in this paper, through the implementation of our proposed model, \textit{speech intelligibility} is interpreted as a latent trait of individuals (hypothetical construct), which underlies the probability of observing a set of entropy replicates; that in turns, describes the ability of transcribers to identify the words in an utterance. Henceforth, statements such \textit{`speech intelligibility is influenced by'} can be read as \textit{`the probability of observing a set of entropy replicates for each individual in the sample is influenced by'}. Similar interpretation can be extended to the (latent) ``true'' entropy measures.

Despite this practical approach, we emphasize we did our best to ensure the construct validity of our study, by ensuring the transcription task was well understood and appropriately performed by the transcribers.

We then expect speech intelligibility, as measured by our model, to reflect the (general) intelligibility of speech possessed by individuals, but do not deal with general epistemological considerations on the connection between the two.
%
%
%###############################
\subsection{Causal framework details} \label{sSA:causal_details}
%###############################
%
In this section we make explicit some of the assumptions that guided our causal framework, and later, our statistical analysis.

For \textbf{\textit{hearing age}}, and its relevance in our research hypothesis, we describe how the variable is constructed and its inherent assumptions. 

\textit{Hearing age} is a composite variable constructed by combining the \textit{chronological age} for the NH group, and the \textit{device length of use} for the HI/CI group \citep{Faes_et_al_2021} (see Table \ref{tab:characteristics}). The variable tries to approximate the amount of time a child has been actively hearing and developing his(her) language. However, no short of evidence has been presented in favor of using others surrogate measures, like \textit{chronological age} \cite{Flipsen_et_al_2006, Habib_et_al_2010, Grandon_et_al_2020} or \textit{age at implantation} \cite{Niparko_et_al_2010, Boons_et_al_2012, Bruijnzeel_et_al_2016, Dettman_et_al_2016}. We argue that the feasibility of using any proxy measure, largely depends on the assumed reliability of the surrogate to approximate the variable of interest. In that sense, although we recognize \textit{hearing age} is not a ``perfect'' proxy \cite{Faes_et_al_2021}, we argue is the most appropriate to test our hypothesis, based on the relevant literature review and its assumed reliability to capture children's language development (although the latter has not been tested). Moreover, the variable serve two additional purposes: (i) control for sampling bias (see section \ref{sSA:sampling_bias}), and (ii) de-confound the parameter estimates of \textit{hearing status} \cite{Cinelli_et_al_2021}.

Finally, it is important to highlight that for modeling purposes, using more than one of the aforementioned proxies in tandem is not recommended. It is apparent from the previous description, the three surrogate measures share high similarities in their data construction. This in turn might cause problems in the modeling procedure, as including variables that provide ``similar information'' might lead to a problem known as multicollinearity, in which our estimates would be biased and less precise \cite{Farrar_et_al_1967}, leading us to wrong conclusions.
%
\begin{comment}
	for the NH group uses the child's \textit{age} (at recording), the method cannot use the same variable for the other two groups. This is due to the fact that \textit{age} is merely used as a proxy, for the amount of time a child has been developing his(her) language. In that sense, more appropriate variables to use under the HI/CI group would be e.g. the \textit{device length of use}, which approximates the ``hearing age'' of such children, or their \textit{vocabulary size}, which resembles their "lexical age" \citep{Faes_et_al_2021}. For this research, we consider the \textit{device length of use} as the simplest one to implement.
\end{comment}

Regarding \textbf{\textit{hearing status}}, it is clear that its inclusion directly corresponds with the main purpose of the current research endeavor. i.e. compare the levels of speech intelligibility among NH and HI/CI children.

In the case of \textbf{\textit{pure tone average}}, the variable was included for two reasons. First, given that previous modeling efforts did not capture the full hierarchy of variability, it is possible that the effects of PTA on speech intelligibility has been largely overlooked. As one can infer, it might be sensible to think that HI/CI children with severe hearing loss, as accounted by the variable, might develop their language at a slower rate. This is especially true, if we consider the signal provided by the cochlear implant is still degraded compared to the signal in normal hearing scenarios \cite{Drennan_et_al_2008}. Finally, as with \textit{hearing age}, the variable might be useful to de-confound the parameter estimates of \textit{hearing status} \cite{Cinelli_et_al_2021}.

For \textbf{\textit{etiology}}, it is possible that its effects on speech intelligibility has also been largely overlooked, due to the lack of control on the full hierarchy of variability, similar to the \textit{pure tone average} case. Moreover, as with its predecessors, the variable might also be useful to de-confound the parameter estimates of \textit{hearing status} \cite{Cinelli_et_al_2021}, assuming our DAG is appropriate.

Finally, it is important to highlight the reason for the absence of other variables in our causal hypothesis.

For the \textbf{\textit{type of cochlear implantation}}, i.e. bilateral or contralateral, the variable was not included because we did not expect it to be related to other variables in the DAG, i.e. the decision on receiving one or the other is solely based on the intelligibility outcome, no matter how it is measured. This in turn means that its inclusion/exclusion would not confound our estimates. Additionally, given that most of the children underwent though sequential bilateral implantation (eleven in total), we anticipated the effect of variable already permeates the sample, therefore, if we wanted to investigate its effect, a larger sample size would be required.

For the case of \textbf{\textit{additional disabilities}}, e.g. mental retardation or speech motor problems, there was no need to consider it, as no additional comorbidities were reported.

In the case of \textbf{\textit{environmental factors}}, such as communication modality, the current sample of HI/CI children were raised orally using monolingual Dutch, with a limited support of signs, a scenario similar to the NH group (see section \ref{sS:children}).

Last but not least, \textbf{\textit{gender}} was not included in our hypothesis as no theoretical nor empirical evidence have been found on its effects \cite{Boonen_et_al_2021}.
%
%
%###############################
\subsection{Sampling bias} \label{sSA:sampling_bias}
%###############################
%
As it happens in most observational, and some experimental studies, ours can also be a potential victim of sampling bias. While stratifying on the selection variables can help to balance the samples, and even ``correct'' the estimates \cite{Cinelli_et_al_2021, Deffner_et_al_2022}, as we do here by controlling for \textit{hearing age}. Given the sample's selection and matching procedures, we cannot ensure the HI/CI nor the NH groups are representative of their respective populations. 

Nevertheless, we argue that by controlling for other relevant confounders, the qualitative results presented in this study holds. However, we cannot discard the presence of unobservable variables that could bias our results, and in that sense, inferences beyond this particular set of children must be taken with care.
%
%
%###############################
\subsection{Model details} \label{sSA:model_details}
%###############################
%
%-------------------------------
\subsubsection{Variability in the beta-proportion distribution} \label{ssSA:model_variability}
%-------------------------------
%
Figure \ref{fig:BetaProp} shows the implications of different ``sample sizes'' ($M_{i}$), on the dispersion of the beta-proportion distribution \cite{Kruschke_2015}. The panels show different ``average'' entropies: the middle panel assumes an ``average'' entropy $\mu=0.5$, the left $\mu=0.2$, and the right $\mu=0.8$ (as shown by the discontinuous lines).

In all three panels we notice two prevalent pattern: (i) the higher the ``sample size'', the less dispersed is the distribution, and (ii) as expected from non-linear models, the behavior of the dispersion depends on the location of the distribution, i.e. their ``average'' value.

In this sense, we expect that if the posterior estimates for $M_{i}$ reach lower values, it would imply the entropy replicates $H^{O}_{bik}$ have high dispersion; and therefore, we could say the replicates have low reliability to reflect the \textit{true} entropy. In contrast, the higher the values for $M_{i}$, the lower the dispersion, and higher the reliability of the replicates to reflect the \textit{true} entropy. 

Then, as one can notice, the approach uses the ``sample size'' parameter to model the replicates' heterogeneity, effectively estimating an entropy's measurement error model \cite{Carroll_2006}.
%
\begin{figure} [!h]
	\centering
	\includegraphics[width=1\linewidth]{BetaProp_dist2.pdf}
	\caption[Variability in a beta-proportional distribution]{Variability in a beta-proportional distribution. Discontinuous lines describe the ``average'' value for the distribution ($\mu$), solid lines describe the distribution assuming different $\theta = M_{ik}$.}
	\label{fig:BetaProp}
\end{figure}
%
%
%-------------------------------
\subsubsection{Data pre-processing} \label{ssSA:preprocessing}
%-------------------------------
%
Besides the exclusion of corrupted observations, e.g. no available transcription, no other information was excluded before the modeling process. 

This decision departs from what has been done in previous research \cite{Boonen_et_al_2020, vanDaal_2020, Boonen_et_al_2021}, and the reason for it, is that the identification of influential observations cannot be done outside the context of a full model \citep{McElreath_2020}, i.e. what can behave as an \textit{outlier} based on a univariate analysis, might behave as expected under the appropriate model. Therefore, we believe the use of preliminary or univariate procedures might lead to erroneous identification of \textit{outliers} and biased conclusions.

Considering the previous, we proposed a set of models that were \textit{robust} against influential observations, rather than to eliminate them based on other analysis (see Table \ref{tab:models}). Playing on the strengths of the Bayesian framework, the proposal of such model was a trivial task. 
%
%
%-------------------------------
\subsubsection{Priors}
%-------------------------------
%

We determined priors for all models through prior predictive simulation. The resulting priors are weakly informative, specifying low probability for impossible outcomes and inducing regularized estimates, but allowing a very wide range of results. The priors used for the model are

Third, we use mildly informative priors and hyper-priors to state our uncertainty regarding the direction and magnitude of the effects\footnote{see \citet{Rivera_2021} (p. 18-19) for an intuition on prior elicitation.}. 
%
\begin{align}
	%
	M_{ik} & \sim \text{LN}( \mu_{M}, \sigma_{M}) \\
	%
	a_{i} & \sim \text{N}(\mu_{a}, \sigma_{a}) \\
	%
	\alpha & \sim \text{N}(0, 0.3) \\
	%
	\alpha_{E[i],HS[i]} & \sim \text{N}(0, 0.3) \\
	%
	\beta_{A, HS[i]} & \sim \text{N}(0 , 0.3) \\
	%
	\beta_{P} & \sim \text{N}(0, 0.3) \\
	%
	\mu_{M} & \sim \text{N}(0, 5) \\
	%
	\sigma_{M} & \sim \text{Exp}(1) \\
	%
	\mu_{a} & \sim \text{N}(0, 0.5) \\
	%
	\sigma_{a} & \sim \text{Exp}(1)
	%
\end{align}
%
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.6\linewidth]{prior_predictive.pdf}
	\caption[Prior distribution implications]{Prior distribution implications: speech intelligibility, \textit{true} entropy and observed entropy scales.}
	\label{fig:priors}
\end{figure}
%
%
%-------------------------------
\subsubsection{Estimation} \label{ssSA:model_estimation}
%-------------------------------
The proposed models, described in section \ref{sS:stat_analysis}, were estimated under the Bayesian framework\footnote{see \citet{Rivera_2021} (p. 11-13, 15-27) for a detailed description of its benefits and shortcomings.}. More specifically, we used the No-U-Turn Hamiltonian Monte Carlo algorithm (No-U-Turn HMC) \citep{Betancourt_et_al_2013, Duane_et_al_1987, Hoffman_et_al_2014, Neal_2012} implemented in \texttt{Stan} \citep{Stan_2020}. Additionally, we used \texttt{R} \citep{R_2015} and its integration packages \citep{RStan_2020} to analyze its outputs.
%
%
%-------------------------------
\subsubsection{Simulation} \label{ssSA:model_simulation}
%-------------------------------
Preliminary to the data collection, we simulated data in silico to test the models and inform data collection procedure. The simulation code is available in the GitHub repository. \cite{Fogarty_et_al_2022}
\begin{comment}
Several functional correlation between age and knowledge have been simulated, and the model used in the analysis - which includes age as a ordinal categorical predictor of knowledge with monotonically increasing effect - has been able to recover the different shapes. Causal effect of activities, family composition and schooling have been simulated and tested.

The simulated data have been used -albeit in a previous version- to estimate the minimum number of interviewees necessary to recover the parameter values. If individuals were to name a maximum of 300 items in the freelist, 50 interviewees would have been suffcient to obtain reliable estimates of the parameters. Given that data collection in vivo is much less regular and less controllable than in silico, we roughly doubled the number of interviewees and that of questions.
\end{comment}
%
%
%-------------------------------
\subsubsection{Model selection} \label{ssSA:model_selection}
%-------------------------------
%
Following the successful and comprehensive analysis in \citet{vanDaal_2020} and \citet{Lesterhuis_2018}, the current research will also use the Information-Theoretic Approach (ITA) \citep{Anderson_2008, Chamberlain_1965} for the selection of competing models. The approach considers three steps: (1) state our hypothesis into statistical models, (2) select among competing models, and (3) make inferences based on one or multiple models.

First, for the translation of our working hypotheses into statistical models, we will use Directed Acyclic Graphs (DAG) and probabilistic programming \citep{Jaynes_2003}. A DAG is the simplest representation of a Graphical Causal Model (GCM), a heuristic model that contains information not purely statistical, but unlike a detailed statistical model, it allow us to deduce which variable relationships can provide valid causal inferences \citep{Hernan_et_al_2020, McElreath_2020}. In summary, a DAG is a reasonable way to state our hypothesis, and make our assumption more transparent. However, abide by the \textit{no-free lunch} rule, the causal inferences produced under the DAG will only be valid if the assumed DAG is correct. In contrast, the probabilistic programming will serve as the algebraic formalist to define our statistical models.

Second, to select among competing models, we will use the Widely Applicable Information Criterion (WAIC) \citep{Watanabe_2013}, and the Pareto-smoothed importance sampling cross-validation (PSIS) \citep{Vehtari_et_al_2021}\footnote{\citet{vanDaal_2020} used the Akaike’s Information Criterion (AIC) \citep{Akaike_1974} with similar purposes.}. Two reasons justify our decision. First, both criteria allow us to embrace the full flexibility and information of our bayesian implementation (outlined in Section \ref{s_sect:models}). Last, and more important, both criteria provide us with the best approximations for the out-of-sample (cross-validated) deviance \citep{McElreath_2020}. The deviance is the best approximation for the Kullback-Liebler (KL) divergence \citep{Kullback_et_al_1951}, i.e. a measure of how far a model is from describing the \textit{true} distribution of our data. \citet{McElreath_2020} points out that is a rather benign characteristic of the model's selection procedure that we do not need the KL divergence's absolute value, as the \textit{true} distribution of our data is not available (otherwise, we would not need a statistical model). But rather, using the difference in deviance between competing models, we can measure which model is the farthest from \textit{perfect (predictive) accuracy} for our data\footnote{see \citet{McElreath_2020} (p. 202-211) for the intuition and detailed derivation of the argument.}.

Finally, considering the evidence provided by the previous step, we proceed to make inferences based on one or multiple models.
%
%